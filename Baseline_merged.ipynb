{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Introduction](#1.-introduction)  \n",
    "&emsp;&emsp;&emsp;&emsp;[1.1. Virtual environment setup](#1.1.-virtual-environment-setup)  \n",
    "&emsp;&emsp;&emsp;&emsp;[1.2. Libraries import](#1.2.-libraries-import)  \n",
    "&emsp;&emsp;&emsp;&emsp;[1.3. Paths definition](#1.3.-paths-definition)  \n",
    "&emsp;&emsp;&emsp;&emsp;[1.4. Functions definition](#1.4.-functions-definition)  \n",
    "[2. Data handling](#2.-data-handling)  \n",
    "&emsp;&emsp;&emsp;&emsp;[2.1. Data import TCN FOR U](#2.1.-data-import-tcn-for-u)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.1.1. Import text files](#2.1.1.-import-text-files)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.1.2. Load the other data](#2.1.2.-load-the-other-data)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.1.3. Data types](#2.1.3.-data-types)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.1.3.1. Check the types](#2.1.3.1.-check-the-types)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.1.3.2. Change the types](#2.1.3.2.-change-the-types)  \n",
    "&emsp;&emsp;&emsp;&emsp;[2.2. Data exploration CCT FOR U](#2.2.-data-exploration-cct-for-u)  \n",
    "&emsp;&emsp;&emsp;&emsp;[2.3. Data combination](#2.3.-data-combination)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.3.1. Preprocessing](#2.3.1.-preprocessing)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.3.1.1. Standardize ratings_BA and ratings_RB annually](#2.3.1.1.-standardize-ratings_ba-and-ratings_rb-annually)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.3.1.2. Standardization of the matched' ratings](#2.3.1.2.-standardization-of-the-matched'-ratings)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.3.2. Summaries standardized data](#2.3.2.-summaries-standardized-data)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.3.3. Concatenation of the dataframes](#2.3.3.-concatenation-of-the-dataframes)  \n",
    "[3. Data Analysis](#3.-data-analysis)  \n",
    "&emsp;&emsp;&emsp;&emsp;[3.1. Numerical ranking TCN FOR U](#3.1.-numerical-ranking-tcn-for-u)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.1. Filtering of the data](#3.1.1.-filtering-of-the-data)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.2. Weighted ranking](#3.1.2.-weighted-ranking)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.2.1. All-time weighted ranking](#3.1.2.1.-all-time-weighted-ranking)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.2.2. Weighted ranking per year](#3.1.2.2.-weighted-ranking-per-year)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.3. Quality-based ranking](#3.1.3.-quality-based-ranking)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.3.1. All time quality ranking](#3.1.3.1.-all-time-quality-ranking)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.3.2. Quality ranking per year](#3.1.3.2.-quality-ranking-per-year)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.4. Evolution of the best breweries grade across the years](#3.1.4.-evolution-of-the-best-breweries-grade-across-the-years)  \n",
    "&emsp;&emsp;&emsp;&emsp;[3.2. Link between polarity, subjectivity of the reviews and ratings CCT FOR U](#3.2.-link-between-polarity,-subjectivity-of-the-reviews-and-ratings-cct-for-u)  \n",
    "&emsp;&emsp;&emsp;&emsp;[3.3. Advanced ranking](#3.3.-advanced-ranking)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.3.1. Reviews' preprocessing](#3.3.1.-reviews'-preprocessing)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.3.2. Calculations of polarity and subjectivity of the reviews with NLP analysis](#3.3.2.-calculations-of-polarity-and-subjectivity-of-the-reviews-with-nlp-analysis)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.3.3. Analysis with the new ranking system](#3.3.3.-analysis-with-the-new-ranking-system)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.3.3.1. New ranking system](#3.3.3.1.-new-ranking-system)  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.3.3.2. Best breweries in the world around the years](#3.3.3.2.-best-breweries-in-the-world-around-the-years)  \n",
    "[4. Annexes](#4.-annexes)  \n",
    "&emsp;&emsp;&emsp;&emsp;[4.1. Generation of the table of content](#4.1.-generation-of-the-table-of-content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Virtual environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A NE PAS OUBLIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import os\n",
    "import warnings\n",
    "import spacy\n",
    "\n",
    "from textblob import TextBlob\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# statistics\n",
    "from scipy.stats import ttest_rel, ttest_ind\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "warnings.filterwarnings('ignore') # remove the warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Paths definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mettre les paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function is defined to save the plots in .html and .png format\n",
    "def plot_format (fig, name):\n",
    "    fig.write_html(output_plot_folder + name + \".html\")\n",
    "    fig.write_image(output_img_folder + name + \".png\", engine=\"kaleido\")\n",
    "\n",
    "    #Show plot image in notebook\n",
    "    return Image(output_img_folder + name + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also accommodates inherent differences between the two websites, such as additional review columns for BeerAdvocate.\n",
    "\n",
    "Once loaded into DataFrames, we utilize the sample_data function to randomly sample a fraction of the data, enabling efficient handling of the substantial dataset.\n",
    "\n",
    "As we've seen in Milestone P2, loading directly the \"rating.txt\" with a standard pipeline is impractical due to laptop memory limitations. We thus splitted the .txt files into N subfiles and load the data iteratively using a function.\n",
    "\n",
    "The function allows control over the fraction of initial data loaded via the parameter **Nb_files**, which determines the number of subfiles to load. It also accommodates inherent differences between the two websites, such as additional review columns for BeerAdvocate. We'd like to load the maximal fraction of intial data thus the maximum number of files\n",
    "\n",
    "Thanks to this loading strategy, we managed to load all the data from the .txt file in dataframe instance without having to sample element from those dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Import text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_file(website, Nb_files):\n",
    "    if website == 'BeerAdvocate':\n",
    "        # Specify the directory containing the text files \n",
    "        # Need to be outside of the repo folder and might need to change the path according to one's unique folders configuration\n",
    "        directory = './../../dataset_BeerReviews/BeerAdvocate/ratings_split_BA/'\n",
    "    else:\n",
    "        directory = './../../dataset_BeerReviews/RateBeer/ratings_split_RB/'\n",
    "        \n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through the files in reverse order (ratings-5.txt to ratings-1.txt)\n",
    "    for i in range(Nb_files, 0, -1):\n",
    "        file_name = f'ratings-{i}.txt'\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Remove double quotes at the beginning of each line\n",
    "        data = re.sub('\"', '', text)\n",
    "        data = re.sub(r'^\"', '', data, flags=re.MULTILINE)\n",
    "\n",
    "        # Split the text into individual beer reviews\n",
    "        beer_reviews = data.split('beer_name')\n",
    "\n",
    "        # Extract the beer information from each review\n",
    "        beer_data = []\n",
    "        for review in beer_reviews:\n",
    "            beer_info = {}\n",
    "            for line in [entry.split(':', 1) for entry in review.split('\\n') if ':' in entry]:# and 'text' not in entry]:\n",
    "                if line:  # Check if the list is not empty\n",
    "                    key, value = line[0].strip(), line[1].strip()\n",
    "                    beer_info[key] = value\n",
    "            beer_data.append(beer_info)\n",
    "\n",
    "        # Convert the beer data into a DataFrame\n",
    "        df = pd.DataFrame(beer_data)\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Rename the columns depending on the website\n",
    "    if website == 'BeerAdvocate':\n",
    "        final_df.columns = [\n",
    "            'beer_name',\n",
    "            'beer_id',\n",
    "            'brewery_name',\n",
    "            'brewery_id',\n",
    "            'style',\n",
    "            'abv',\n",
    "            'date',\n",
    "            'user_name',\n",
    "            'user_id',\n",
    "            'appearance',\n",
    "            'aroma',\n",
    "            'palate',\n",
    "            'taste',\n",
    "            'overall',\n",
    "            'rating',\n",
    "            'text',\n",
    "            'review'] # additional column compared to RB\n",
    "    else:\n",
    "        final_df.columns = [\n",
    "            'beer_name',\n",
    "            'beer_id',\n",
    "            'brewery_name',\n",
    "            'brewery_id',\n",
    "            'style',\n",
    "            'abv',\n",
    "            'date',\n",
    "            'user_name',\n",
    "            'user_id',\n",
    "            'appearance',\n",
    "            'aroma',\n",
    "            'palate',\n",
    "            'taste',\n",
    "            'overall',\n",
    "            'rating',\n",
    "            'text']\n",
    "    return final_df\n",
    "\n",
    "def sample_data(df, ratio):\n",
    "    # sample the data to make it even smaller\n",
    "    return df.sample(frac = ratio, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data : for faster processing, we will only load 1 file out of the whole dataset\n",
    "ratings_BA = load_txt_file('BeerAdvocate', 2)\n",
    "ratings_RB = load_txt_file('RateBeer', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data to make it even smaller AND RANDOMIZED (not randomized in the initial txt splitting process)\n",
    "ratings_BA = sample_data(df = ratings_BA, ratio = 0.25)\n",
    "ratings_RB = sample_data(df = ratings_RB, ratio = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of ratings_BA (BeerAdvocate) dataframe : {len(ratings_BA)}\")\n",
    "print(f\"Length of ratings_RB (RateBeer) dataframe : {len(ratings_RB)}\")\n",
    "display(ratings_BA.head(2),ratings_RB.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Load the other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeerAdvocate\n",
    "beers_BA = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/BeerAdvocate/beers.csv\")\n",
    "breweries_BA = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/BeerAdvocate/breweries.csv\")\n",
    "users_BA = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/BeerAdvocate/users.csv\")\n",
    "\n",
    "# RateBeer\n",
    "beers_RB = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/RateBeer/beers.csv\")\n",
    "breweries_RB = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/RateBeer/breweries.csv\")\n",
    "users_RB = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/RateBeer/users.csv\")\n",
    "\n",
    "# matched_beer_data\n",
    "beers_matched = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/matched_beer_data/beers.csv\", header=1)\n",
    "breweries_matched = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/matched_beer_data/breweries.csv\", header = 1)\n",
    "ratings_matched = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/matched_beer_data/ratings.csv\", header=1)\n",
    "users_approx = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/matched_beer_data/users_approx.csv\", header=1)\n",
    "users_matched = pd.read_csv(\"./../ada-2023-project-laremontada61/dataset_BeerReviews/matched_beer_data/users.csv\", header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.1. Check the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get types of features in each DataFrame\n",
    "df_types = pd.concat([df.dtypes.rename(f\"DataFrame {i+1}\") for i, df in enumerate([ratings_BA, ratings_RB])], axis=1)\n",
    "df_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.2. Change the types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that all the features of **ratings_BA** and **ratings_RB** have a `object` type, which is not very convenient if we want to automate the visualization process, for example plotting the distribution of the numerical features in histograms. As a first preprocessing step, we will thus convert the type of the numerical variables to `float64` with the function **convert_type** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_type(df, col):\n",
    "    df[col] = df[col].astype('float64')\n",
    "    return df\n",
    "\n",
    "# columns including the numerical features for both websites\n",
    "# user_id is numerical in RB but categorical in BA so we don't include it\n",
    "numerical_cols = ['beer_id', 'brewery_id', 'abv', 'appearance', 'aroma', 'palate', 'taste', 'overall', 'rating']\n",
    "\n",
    "for col in numerical_cols : \n",
    "    ratings_BA = convert_type(ratings_BA, col)\n",
    "    ratings_RB = convert_type(ratings_RB, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get types of features in each DataFrame\n",
    "df_types = pd.concat([df.dtypes.rename(f\"DataFrame {i+1}\") for i, df in enumerate([ratings_BA, ratings_RB])], axis=1)\n",
    "df_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Data exploration CCT FOR U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Data combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matched (already standardized, need to check scales though of numerical features)\n",
    "- BA (not standardized yet)\n",
    "- RB (not standardized yet)\n",
    "\n",
    "First we need to standardize the data from ratings_BA and ratings_RB before combining them with matched (which has already been standardized per year and per website, see paper)\n",
    "\n",
    "Explain that we duplicate the matched samples (1 from BA and 1 from RB), so that we keep all the reviews, independently of the website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.1. Standardize ratings_BA and ratings_RB annually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need to add time dimension first: month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    \"\"\"\n",
    "    Adds a 'month' and a 'year' features to the DataFrame based on the 'date' column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Contains data including a 'date' column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_m : DataFrame\n",
    "        Contains the original data with an additional 'month' and 'year' features.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame\n",
    "    df_time = df.copy()\n",
    "    # Conversion of date feature from timestamp to text date\n",
    "    pd.to_numeric(df_time['date'], errors='coerce', downcast='integer')\n",
    "    # Convert 'date' to datetime and extract the date\n",
    "    df_time.date = df.date.apply(lambda d: pd.to_datetime(d, unit='s'))\n",
    "    df_time['date'] = df_time['date'].dt.date\n",
    "\n",
    "    # Extract the month from the 'date' column and add it as a new feature 'month'\n",
    "    df_time['month'] = pd.to_datetime(df_time['date']).dt.month\n",
    "    # Extract the year from the 'date' column and add it as a new feature 'year'\n",
    "    df_time['year'] = pd.to_datetime(df_time['date']).dt.year\n",
    "\n",
    "    return df_time\n",
    "\n",
    "def standardize_annually_ratings(df):\n",
    "    \"\"\"\n",
    "    Standardizes the ratings annually and per website.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Containing all data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        Containing all data with standardized ratings with additional column 'standardized_rating'.\n",
    "                \n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    grouped = df_copy.groupby('year')['rating'].agg(['mean', 'std'])\n",
    "    df_copy = pd.merge(df_copy, grouped, left_on='year', right_index=True)\n",
    "    df_copy['std_rating'] = (df_copy['rating'] - df_copy['mean']) / df_copy['std']\n",
    "    \n",
    "    return df_copy, grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time feature to standardize ratings by year and by website\n",
    "ratings_BA_with_time = add_time_features(ratings_BA)\n",
    "ratings_RB_with_time = add_time_features(ratings_RB)\n",
    "\n",
    "# apply standardization\n",
    "ratings_BA_standardized, mean_std_BA = standardize_annually_ratings(ratings_BA_with_time)\n",
    "ratings_RB_standardized, mean_std_RB = standardize_annually_ratings(ratings_RB_with_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_std_errorbar(grouped_BA, grouped_RB):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.errorbar(grouped_BA.index, grouped_BA['mean'], yerr=1.96 * grouped_BA['std'] / np.sqrt(grouped_BA['std'].count()), fmt='o', capsize=5, linestyle='-', label = 'BeerAdvocate')\n",
    "    plt.errorbar(grouped_RB.index, grouped_RB['mean'], yerr=1.96 * grouped_RB['std'] / np.sqrt(grouped_RB['std'].count()), fmt='o', capsize=5, linestyle='-', label = 'RateBeer')\n",
    "    plt.title('Mean Ratings across Years')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.errorbar(grouped_BA.index, grouped_BA['std'], yerr=1.96 * grouped_BA['std'] / np.sqrt(grouped_BA['std'].count()), fmt='o', capsize=5, linestyle='-', label = 'BeerAdvocate')\n",
    "    plt.errorbar(grouped_RB.index, grouped_RB['std'], yerr=1.96 * grouped_RB['std'] / np.sqrt(grouped_RB['std'].count()), fmt='o', capsize=5, linestyle='-', label = 'RateBeer')\n",
    "    plt.title('Standard Deviation of Ratings across Years')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before standardization\")\n",
    "plot_mean_std_errorbar(mean_std_BA, mean_std_RB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std_BA_standardized = ratings_BA_standardized.groupby('year')['std_rating'].agg(['mean', 'std'])\n",
    "mean_std_RB_standardized = ratings_RB_standardized.groupby('year')['std_rating'].agg(['mean', 'std'])\n",
    "print(\"After standardization\")\n",
    "plot_mean_std_errorbar(mean_std_BA_standardized, mean_std_RB_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.histplot(data=ratings_matched['rating'], bins=50, label = 'ratings_BA_matched')\n",
    "sns.histplot(data=ratings_matched['rating.1'], bins=50, label = 'ratings_RB_matched')\n",
    "sns.histplot(data=ratings_BA_standardized['std_rating'], bins=50, label = 'ratings_BA_standardized')\n",
    "sns.histplot(data=ratings_RB_standardized['std_rating'], bins=50, label = 'ratings_RB_standardized')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.2. Standardization of the matched' ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matched_copy = ratings_matched.copy(deep=True)\n",
    "ratings_matched_copy['beer_id.1'] = ratings_matched_copy['beer_id']\n",
    "ratings_matched_copy['brewery_id.1'] = ratings_matched_copy['brewery_id']\n",
    "ratings_matched_copy['beer_name.1'] = ratings_matched_copy['beer_name']\n",
    "ratings_matched_copy['brewery_name.1'] = ratings_matched_copy['brewery_name']\n",
    "\n",
    "# Retrieve the columns of interest for BeerAdvocate\n",
    "ratings_matched_BA = ratings_matched_copy.iloc[:, :17]\n",
    "ratings_matched_BA = ratings_matched_BA.drop(['review'], axis=1) # drop the column that is not common to both websites\n",
    "\n",
    "#Retrieve the columns of interest for RateBeer\n",
    "ratings_matched_RB = ratings_matched_copy.iloc[:, 17:]\n",
    "ratings_matched_RB['overall.1'] = ratings_matched_RB['overall.1']*5/20\n",
    "ratings_matched_RB['aroma.1'] = ratings_matched_RB['aroma.1']*5/10\n",
    "ratings_matched_RB['taste.1'] = ratings_matched_RB['taste.1']*5/10\n",
    "ratings_matched_RB.columns = ratings_matched_BA.columns\n",
    "\n",
    "# add time features to standardize by year\n",
    "ratings_matched_BA_with_time = add_time_features(ratings_matched_BA)\n",
    "ratings_matched_RB_with_time = add_time_features(ratings_matched_RB)\n",
    "\n",
    "# standardize ratings of matched data to have mean 0 and std 1\n",
    "ratings_matched_BA_standardized, mean_std_BA_matched = standardize_annually_ratings(ratings_matched_BA_with_time) # adds column 'std_rating'\n",
    "ratings_matched_RB_standardized, mean_std_RB_matched = standardize_annually_ratings(ratings_matched_RB_with_time) # adds column 'std_rating'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Summaries standardized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dataframes have an additional column 'std_rating' with the standardized rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean values for each distribution\n",
    "mean_rating_BA_matched = ratings_matched_BA_standardized['std_rating'].mean()\n",
    "mean_rating_RB_matched = ratings_matched_RB_standardized['std_rating'].mean()\n",
    "mean_rating_BA_standardized = ratings_BA_standardized['std_rating'].mean()\n",
    "mean_rating_RB_standardized = ratings_RB_standardized['std_rating'].mean()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.histplot(data=ratings_matched_BA_standardized['std_rating'], bins=50, label = 'ratings_BA_matched_standardized')\n",
    "sns.histplot(data=ratings_matched_RB_standardized['std_rating'], bins=50, label = 'ratings_RB_matched_standardized')\n",
    "sns.histplot(data=ratings_BA_standardized['std_rating'], bins=50, label = 'ratings_BA_standardized')\n",
    "sns.histplot(data=ratings_RB_standardized['std_rating'], bins=50, label = 'ratings_RB_standardized')\n",
    "\n",
    "# Plot vertical lines for mean values\n",
    "plt.axvline(x=mean_rating_BA_matched, color='green', linestyle='--', label=f'Mean BA Matched: {mean_rating_BA_matched:.2f}')\n",
    "plt.axvline(x=mean_rating_RB_matched, color='blue', linestyle='--', label=f'Mean RB Matched: {mean_rating_RB_matched:.2f}')\n",
    "plt.axvline(x=mean_rating_BA_standardized, color='orange', linestyle='--', label=f'Mean BA Standardized: {mean_rating_BA_standardized:.2f}')\n",
    "plt.axvline(x=mean_rating_RB_standardized, color='red', linestyle='--', label=f'Mean RB Standardized: {mean_rating_RB_standardized:.2f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Concatenation of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate matched data from both websites\n",
    "ratings_matched_BA_RB = pd.concat([ratings_matched_BA_standardized, ratings_matched_RB_standardized], axis=0)\n",
    "ratings_matched_BA_RB.sort_values(by=['beer_id', 'user_id'], inplace=True)\n",
    "\n",
    "# concatenate matched data with ratings_BA and ratings_RB\n",
    "all_data= pd.concat([ratings_BA_standardized, \n",
    "                    ratings_RB_standardized,\n",
    "                    ratings_matched_BA_RB], axis=0)\n",
    "\n",
    "# test\n",
    "print(f\"Length of ratings matched: {len(ratings_matched)}\")\n",
    "print(f\"Length of ratings BA: {len(ratings_BA)}\")\n",
    "print(f\"Length of ratings RB: {len(ratings_RB)}\")\n",
    "print(f\"Length of combined dataset: {len(all_data)}\")\n",
    "\n",
    "# check\n",
    "sns.histplot(data=all_data['std_rating'], bins=50)\n",
    "plt.show()\n",
    "\n",
    "all_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Numerical ranking TCN FOR U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will construct two ranking of the best breweries by grading them based on their beers grade. We filter out the breweries with too few reviews to maintain the ranking credibility and ensure that the rating reflects a consensus among significant number of reviewers. This minimum number of review is set to 30 review, This threshold gives us a robust ranking. We tested with some lower threshold for instance 10 or 15, in those cases the best brewery contained 16 beers, only one of its beer had more than 5 review, which is not significant enough. Its average was very high due to the small number of reviews. As we will see later, a threshold of 30 gives a more robust ranking because the breweries have a satisfactory number of reviews. In both ranking we start by computing the averages obtained by each of the brewery's beers. Here is the first ranking system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The first ranking is a weighted ranking, when computing the grade of the brewerie we leverage the importance of each beers by the number of reviews it has received. This ranking isn't really sensitive to beers with a low number of reviews we thus don't filter out beers with little number of reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this classification may not be considered representative of a brewery's quality. A brewery could have a very popular beer that is highly rated and drunk a lot, and the rest of its beers that are less highly rated and drunk less. This brewery in the first ranking will obtain a good average. However, the quality of the brewery can be questioned since only one of its beers is well rated. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second ranking takes this effect into account.It's a quality ranking as it doesn't take into account the quantity of review. It calculates a brewery's rating by averaging the beers without taking their frequency into account. This ranking could represent a better ranking of a brewery quality because it values quality among different beers. This ranking is very sensitive to beers that are rated very few times, which is why we have to filter out beers with too few reviews. We have decided that a beer with fewer than 4 reviews cannot be considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we will also explore the evolution of those ranking across the years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Filtering of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first explore the distribution of the number of review per beers or breweries, to have an idea of the impact of our filtering on the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reviews(beers, breweries):\n",
    "    fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "    axs[0].hist(beers,bins=100, log=True,histtype='step')\n",
    "    axs[0].set_title(\"Distribution of the number of review per beer\")\n",
    "    \n",
    "    \n",
    "    axs[1].hist(breweries,bins=100 , log=True,histtype='step')\n",
    "    axs[1].set_title(\"Distribution of the number of review per breweries\")\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rev_beer = all_data.groupby('beer_name').apply(lambda x:len(x))\n",
    "nb_rev_breweries = all_data.groupby('brewery_name').apply(lambda x:len(x))\n",
    "plot_reviews(nb_rev_beer, nb_rev_breweries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many beers are removed when we apply a thresold of 4 reviews per beer for the second rating system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_filtered = all_data.groupby('beer_name').filter(lambda x: len(x)>4)\n",
    "values =sum(1 for element in nb_rev_beer.values if element <= 4)\n",
    "length_beer =len(nb_rev_beer.values)\n",
    "length_review =all_data.shape[0]\n",
    "print(f\"We kicked {values} beeer out of {length_beer} and {all_data.shape[0]-ratings_filtered.shape[0]} out of {all_data.shape[0]} reviews \")\n",
    "print(f\"which is {np.round(values/length_beer*100,3)}% of the beer and {np.round(100*(all_data.shape[0]-ratings_filtered.shape[0])/all_data.shape[0], 3)}% of the reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we filter the breweries with less than 30 reviews and count the propotion of breweries removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Justfier avec graph le threshold brasserie\n",
    "all_data = all_data.groupby('brewery_name').filter(lambda x: len(x)>30)\n",
    "# %%\n",
    "\n",
    "kicked_brew =sum(1 for element in nb_rev_breweries.values if element <= 10)\n",
    "length_brew =len(nb_rev_breweries.values)\n",
    "print(f\"We kicked {kicked_brew} breweries out of {length_brew}\")# and {ratings_BA.shape[0]-length_review} out of {ratings_BA.shape[0]} reviews \")\n",
    "print(f\"which is {np.round(kicked_brew/length_brew*100,3)}% of the breweries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the ranking itself, in order to visualize and compare the results of the two system, we'll need those two plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ranking(ranking_sorted, topN):\n",
    "    plt.figure(figsize=(8, topN/4))\n",
    "    colors = plt.cm.Reds(ranking_sorted[:topN] / ranking_sorted[:topN].max())\n",
    "    plt.barh(ranking_sorted.index[:topN], ranking_sorted[:topN], color=colors)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f'Top {topN} Breweries of all time')\n",
    "    plt.show()\n",
    "    \n",
    "def compare_ranking(ranking_sorted_1, ranking_sorted_2, topN):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    colors_1 = plt.cm.Reds(ranking_sorted_1[:topN] / ranking_sorted_1[:topN].max())\n",
    "    axes[0].barh(ranking_sorted_1.index[:topN], ranking_sorted_1[:topN], color=colors_1)\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[0].set_title(f'Top {topN} Breweries - Weighted Ranking')\n",
    "    \n",
    "    colors_2 = plt.cm.Blues(ranking_sorted_2[:topN] / ranking_sorted_2[:topN].max())\n",
    "    axes[1].barh(ranking_sorted_2.index[:topN], ranking_sorted_2[:topN], color=colors_2)\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].set_title(f'Top {topN} Breweries - Fair Ranking')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is grouped by brewery and by beer. This number gives you the scores for each brewery. In the case of the classification by year, we also take into account the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_brew = all_data.groupby(['brewery_name',\"beer_name\"])[\"std_rating\"]\n",
    "grouped_brew_year = all_data.groupby(['brewery_name',\"beer_name\",\"year\"])[\"std_rating\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Weighted ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_ranking(data , version , col):\n",
    "    if version == \"all\":\n",
    "        grouped_brew = data.groupby(['brewery_name',\"beer_name\"])[col]\n",
    "        grouped_agg = grouped_brew.agg(['mean', 'size'])\n",
    "        grouped_agg['weighted_rating'] = grouped_agg['mean'] * grouped_agg['size']\n",
    "\n",
    "        #Here we calculate the scores by taking into account the nb of ratings \n",
    "        nb_review = grouped_agg.groupby('brewery_name')['size'].sum()#.apply(lambda x : len(x))\n",
    "        weighted_ranking = grouped_agg.groupby('brewery_name')['weighted_rating'].sum() / nb_review\n",
    "        weighted_ranking_sorted = weighted_ranking.sort_values(ascending=False)\n",
    "\n",
    "        print(weighted_ranking_sorted.head())\n",
    "        return weighted_ranking_sorted\n",
    "    else :\n",
    "        grouped_brew_year = data.groupby(['brewery_name',\"beer_name\",\"year\"])[\"std_rating\"]\n",
    "        grouped_agg_year = grouped_brew_year.agg(['mean', 'size'])\n",
    "        grouped_agg_year['weighted_rating'] = grouped_agg_year['mean'] * grouped_agg_year['size']\n",
    "\n",
    "        nb_review = grouped_agg_year.groupby('brewery_name')['size'].sum()\n",
    "        weighted_ranking_year = grouped_agg_year.groupby(['brewery_name',\"year\"])['weighted_rating'].sum() / nb_review\n",
    "        averaged_per_year = weighted_ranking_year.reset_index(level='brewery_name')\n",
    "        averaged_per_year.columns.values[1] = 'std_rating'\n",
    "\n",
    "        top_brewery_year_avg = averaged_per_year.groupby('year').apply(lambda x: x.nlargest(10, 'std_rating'))\n",
    "\n",
    "        top_across_avg = top_brewery_year_avg.groupby(\"brewery_name\").value_counts().nlargest(5)\n",
    "\n",
    "        weighted_ranking_year_sorted = top_brewery_year_avg.groupby(\"year\").first()\n",
    "        print(weighted_ranking_year_sorted.head(10))\n",
    "        return weighted_ranking_year_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.1. All-time weighted ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ranking_sorted = weighted_ranking(all_data,\"all\",\"std_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the n best breweries of all time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ranking(weighted_ranking_sorted, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.2. Weighted ranking per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the same ranking per year and only keep the top ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ranking_year_sorted = weighted_ranking(all_data,\"year\",\"std_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ranking_year_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Quality-based ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_ranking(data , version , col):\n",
    "    ratings_filtered = data.groupby('beer_name').filter(lambda x: len(x)>4)\n",
    "    if version == \"all\":\n",
    "    \n",
    "        grouped_brew = ratings_filtered.groupby(['brewery_name',\"beer_name\"])[col]\n",
    "        grouped_agg = grouped_brew.agg(['mean'])\n",
    "        nb_beer = grouped_agg.groupby('brewery_name').apply(lambda x : len(x))\n",
    "        quality_ranking = grouped_agg.groupby('brewery_name')['mean'].sum() / nb_beer\n",
    "        #fair_ranking = grouped_agg.groupby('brewery_name')['mean'].mean()\n",
    "        quality_ranking_sorted = quality_ranking.sort_values(ascending=False)\n",
    "\n",
    "        print(quality_ranking_sorted.head())\n",
    "        return quality_ranking_sorted\n",
    "    else :\n",
    "        grouped_brew_year = ratings_filtered.groupby(['brewery_name',\"beer_name\",\"year\"])[col]\n",
    "        grouped_agg_year = grouped_brew_year.agg(['mean'])\n",
    "\n",
    "\n",
    "        nb_beer = grouped_agg_year.groupby('brewery_name').apply(lambda x : len(x))\n",
    "        quality_ranking_year = grouped_agg_year.groupby(['brewery_name',\"year\"])['mean'].sum() / nb_beer\n",
    "\n",
    "        averaged_quality_per_year = quality_ranking_year.reset_index(level='brewery_name')\n",
    "        averaged_quality_per_year.columns.values[1] = 'std_rating'\n",
    "\n",
    "        top_brewery_year_quality = averaged_quality_per_year.groupby('year').apply(lambda x: x.nlargest(10, 'std_rating'))\n",
    "        top_across_fair = top_brewery_year_quality.groupby(\"brewery_name\").value_counts().nlargest(5)\n",
    "\n",
    "        quality_ranking_year_sorted = top_brewery_year_quality.groupby(\"year\").first()\n",
    "        quality_ranking_year_sorted.head(10)\n",
    "        return quality_ranking_year_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3.1. All time quality ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this ranking we start by filtering out the beers with less than 4 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_ranking_all = quality_ranking(all_data,\"all\",\"std_rating\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the results of those two systems for the overall ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_ranking(weighted_ranking_sorted, quality_ranking_sorted, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3.2. Quality ranking per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the weighted system let's see the ranking per year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_ranking_year = quality_ranking(all_data,\"year\",\"std_rating\")\n",
    "quality_ranking_year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Evolution of the best breweries grade across the years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we plot the evolution of the rating of our top 3 best brewerie across the year, we decided to plot the ratings not standardized for it to be interpretable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_three = weighted_ranking_sorted.head(1).index.to_list()\n",
    "leader = all_data[all_data[\"brewery_name\"].isin(top_three)]\n",
    "leader_by_year = leader.groupby([\"brewery_name\",'year']).apply(lambda x:pd.Series({\n",
    "        'average_rate': x[\"rating\"].mean(),\n",
    "        'std_rate': x[\"rating\"].std()\n",
    "    }))\n",
    "unstacked = leader_by_year.unstack(level = \"brewery_name\")\n",
    "print(unstacked.index)\n",
    "for brewery in top_three:\n",
    "    plt.errorbar(unstacked.index, unstacked[(\"average_rate\",brewery)],label = brewery)# ,yerr = unstacked[(\"std_rate\",brewery)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Link between polarity, subjectivity of the reviews and ratings CCT FOR U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Advanced ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Reviews' preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('stopwords') # Download NLTK stopwords\n",
    "nltk.download('wordnet')   # Lemmatization\n",
    "nltk.download('words')\n",
    "english_words=set(words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ratings, there are some people that have not made a textual review so we remove them to have equity in our ranking. We keep only english reviews because translating is too much calculative expensive, and require use of Google Translation API. The principal reason is also that 90% of the reviews are in English and the polarity and subjectivity calculation need to be consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " JUSTIFY AVEC PLOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_no_sentences(df):\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df = df[df['text'].str.strip().astype(bool)]  # Check for non-empty strings\n",
    "    df = df[df['text'] != 'nan']\n",
    "    return df\n",
    "def keep_only_english(sentence):\n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(sentence) if w.lower() in english_words or not w.isalpha())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = remove_no_sentences(all_data)\n",
    "all_data['text'] = all_data['text'].apply(lambda x: keep_only_english(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Calculations of polarity and subjectivity of the reviews with NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def getPolarity(text):\n",
    "    return analyzer.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['polarity_text'] = all_data['text'].apply(lambda x: getPolarity(x))\n",
    "all_data['polarity_rating'] = all_data['text'].apply(lambda x: getPolarity(x)['compound'])\n",
    "all_data['subjectivity'] = all_data['text'].apply(lambda x: getSubjectivity(x))\n",
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Analysis with the new ranking system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3.1. New ranking system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " STANDARDIZATION BY YEARS AVEC FONCTION MAXIME ADAPTEE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial rankings are solely based on numerical ratings. To enhance the fairness of our rating system, we have decided to incorporate textual reviews. To achieve this, we will consider the polarity of the comments, representing a score ranging from -1 (very negative) to 1 (very positive). However, it is crucial to acknowledge that individuals may introduce subjectivity into their ratings, denoted on a scale from 0 to 1, where 0 indicates objectivity and 1 signifies high subjectivity.\n",
    "\n",
    "In our quest for a fair ranking system, we aim to mitigate the impact of highly subjective opinions. To accomplish this, we introduce a coefficient (1-subjectivity), ensuring that individuals with more subjective views carry less weight in the final ranking.\n",
    "\n",
    "Additionally, we're streamlining our review process by standardizing all polarity scores together and not website by website as numerical ratings. This means that individuals expressing similar sentiments will receive the same polarity rating, irrespective of variations in their numerical ratings. For example, if two people share the same thoughts and emotions, one giving a rating of 3 and the other a 4, our machine learning system will assign them identical polarity ratings. This simplification ensures coherence and consistency in our overall ranking system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['new_rating'] = (all_data['std_rating'] + all_data['std_polarity_rating'])*(1-all_data['subjectivity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3.2. Best breweries in the world around the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_best_beers = all_data.groupby('beer_name')['new_rating'].mean().nlargest(10)\n",
    "new_best_beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_best_breweries = all_data.groupby('brewery_name')['new_rating'].mean().nlargest(10)\n",
    "new_best_breweries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ratings = all_data.groupby(['beer_name', 'year'])['new_rating'].mean().reset_index()\n",
    "\n",
    "# Obtenir le top 10 des bires pour chaque anne\n",
    "top_10_beers_by_year = (\n",
    "    average_ratings.sort_values(by=['year', 'new_rating'], ascending=[True, False]).groupby('year').head(10).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_10_beers_by_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Annexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Generation of the table of content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import pandas as pd\n",
    "\n",
    "# Load the .ipynb file\n",
    "with open('./Baseline_merged.ipynb', 'r') as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Extract the markdown cells\n",
    "markdown_cells = [cell for cell in notebook['cells'] if cell['cell_type'] == 'markdown']\n",
    "\n",
    "def levelf (lev, niv):\n",
    "    i = lev[0:niv]\n",
    "    string = \"\"\n",
    "    for n in i:\n",
    "        string = string + str(n) + \".\"\n",
    "    string += \" \"\n",
    "    return string\n",
    "\n",
    "#Get all the markdown cells heading names and rename them with a sorted number and change the mardowns names in the notebook\n",
    "index = [0, 0, 0, 0, 0, 0]\n",
    "heading_corr = []\n",
    "for i in range(len(notebook['cells'])):\n",
    "    if notebook['cells'][i]['cell_type'] == 'markdown':\n",
    "        if ('###### ') in notebook['cells'][i]['source'][0:7]:\n",
    "            heading = notebook['cells'][i]['source']\n",
    "            index[5] += 1\n",
    "            level = levelf(index, 6)\n",
    "            notebook['cells'][i]['source'] = '###### ' + level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else '###### ' + level + heading[7:]\n",
    "            head = level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else level + heading[7:]\n",
    "            heading_corr.append([\"<a style='font-size:0.5em'>\" + head + \"</a>\", head, level])\n",
    "        elif ('##### ') in notebook['cells'][i]['source'][0:6]:\n",
    "            heading = notebook['cells'][i]['source']\n",
    "            index[4] += 1\n",
    "            index[5] = 0\n",
    "            level = levelf(index, 5)\n",
    "            notebook['cells'][i]['source'] = '##### ' + level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else '##### ' + level + heading[6:]\n",
    "            head = level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else level + heading[6:]\n",
    "            heading_corr.append([\"<a style='font-size:0.75em'>\" + head + \"</a>\", head, level])\n",
    "        elif ('#### ') in notebook['cells'][i]['source'][0:5]:\n",
    "            heading = notebook['cells'][i]['source']\n",
    "            index[3] += 1\n",
    "            index[4] = 0\n",
    "            index[5] = 0\n",
    "            level = levelf(index, 4)\n",
    "            notebook['cells'][i]['source'] = '#### ' + level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else '#### ' + level + heading[5:]\n",
    "            head = level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else level + heading[5:]\n",
    "            heading_corr.append([\"<a style='font-size:1.25em'>\" + head + \"</a>\", head, level])\n",
    "        elif ('### ') in notebook['cells'][i]['source'][0:4]:\n",
    "            heading = notebook['cells'][i]['source']\n",
    "            index[2] += 1\n",
    "            index[3] = 0\n",
    "            index[4] = 0\n",
    "            index[5] = 0\n",
    "            level = levelf(index, 3)\n",
    "            notebook['cells'][i]['source'] = '### ' + level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else '### ' + level + heading[4:]\n",
    "            head = level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else level + heading[4:]\n",
    "            heading_corr.append([\"<a style='font-size:1.5em'>\" + head + \"</a>\", head, level])\n",
    "        elif ('## ') in notebook['cells'][i]['source'][0:3]:\n",
    "            heading = notebook['cells'][i]['source']\n",
    "            index[1] += 1\n",
    "            index[2] = 0\n",
    "            index[3] = 0\n",
    "            index[4] = 0\n",
    "            index[5] = 0\n",
    "            level = levelf(index, 2)\n",
    "            notebook['cells'][i]['source'] = '## ' + level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else '## ' + level + heading[3:]\n",
    "            head = level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else level + heading[3:]\n",
    "            heading_corr.append([\"<a style='font-size:1.75em'>\" + head + \"</a>\", head, level])\n",
    "        elif ('# ') in notebook['cells'][i]['source'][0:2]:\n",
    "            heading = notebook['cells'][i]['source']\n",
    "            index[0] += 1\n",
    "            index[1] = 0\n",
    "            index[2] = 0\n",
    "            index[3] = 0\n",
    "            index[4] = 0\n",
    "            index[5] = 0\n",
    "            level = levelf(index, 1)\n",
    "            notebook['cells'][i]['source'] = '# ' + level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else '# ' + level + heading[2:]\n",
    "            head = level + heading[heading.rfind(\". \"):][2:] if \". \" in heading else level + heading[2:]\n",
    "            heading_corr.append([\"<a style='font-size:2em'>\" + head + \"</a>\", head, level])\n",
    "\n",
    "# Combine lists\n",
    "heading_corr\n",
    "tab = \"&emsp;\"\n",
    "# Generate the table of contents\n",
    "toc = '\\n'.join(f'{int(len(level)-3)*2*tab}[{ref}](#{ref.lower().replace(\" \", \"-\")})  ' for heading, ref, level in heading_corr) #\n",
    "\n",
    "# Write the updated notebook to disk\n",
    "with open('./Baseline_merged.ipynb', 'w+') as f:\n",
    "    nbformat.write(notebook, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. Introduction](#1.-introduction)  \n",
      "&emsp;&emsp;&emsp;&emsp;[1.1. Virtual environment setup](#1.1.-virtual-environment-setup)  \n",
      "&emsp;&emsp;&emsp;&emsp;[1.2. Libraries import](#1.2.-libraries-import)  \n",
      "&emsp;&emsp;&emsp;&emsp;[1.3. Paths definition](#1.3.-paths-definition)  \n",
      "&emsp;&emsp;&emsp;&emsp;[1.4. Functions definition](#1.4.-functions-definition)  \n",
      "[2. Data handling](#2.-data-handling)  \n",
      "&emsp;&emsp;&emsp;&emsp;[2.1. Data import TCN FOR U](#2.1.-data-import-tcn-for-u)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.1.1. Import text files](#2.1.1.-import-text-files)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.1.2. Load the other data](#2.1.2.-load-the-other-data)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.1.3. Data types](#2.1.3.-data-types)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.1.3.1. Check the types](#2.1.3.1.-check-the-types)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.1.3.2. Change the types](#2.1.3.2.-change-the-types)  \n",
      "&emsp;&emsp;&emsp;&emsp;[2.2. Data exploration CCT FOR U](#2.2.-data-exploration-cct-for-u)  \n",
      "&emsp;&emsp;&emsp;&emsp;[2.3. Data combination](#2.3.-data-combination)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.3.1. Preprocessing](#2.3.1.-preprocessing)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.3.1.1. Standardize ratings_BA and ratings_RB annually](#2.3.1.1.-standardize-ratings_ba-and-ratings_rb-annually)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.3.1.2. Standardization of the matched' ratings](#2.3.1.2.-standardization-of-the-matched'-ratings)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.3.2. Summaries standardized data](#2.3.2.-summaries-standardized-data)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2.3.3. Concatenation of the dataframes](#2.3.3.-concatenation-of-the-dataframes)  \n",
      "[3. Data Analysis](#3.-data-analysis)  \n",
      "&emsp;&emsp;&emsp;&emsp;[3.1. Numerical ranking TCN FOR U](#3.1.-numerical-ranking-tcn-for-u)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.1. Filtering of the data](#3.1.1.-filtering-of-the-data)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.2. Weighted ranking](#3.1.2.-weighted-ranking)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.2.1. All-time weighted ranking](#3.1.2.1.-all-time-weighted-ranking)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.2.2. Weighted ranking per year](#3.1.2.2.-weighted-ranking-per-year)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.3. Quality-based ranking](#3.1.3.-quality-based-ranking)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.3.1. All time quality ranking](#3.1.3.1.-all-time-quality-ranking)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.3.2. Quality ranking per year](#3.1.3.2.-quality-ranking-per-year)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.4. Evolution of the best breweries grade across the years](#3.1.4.-evolution-of-the-best-breweries-grade-across-the-years)  \n",
      "&emsp;&emsp;&emsp;&emsp;[3.2. Link between polarity, subjectivity of the reviews and ratings CCT FOR U](#3.2.-link-between-polarity,-subjectivity-of-the-reviews-and-ratings-cct-for-u)  \n",
      "&emsp;&emsp;&emsp;&emsp;[3.3. Advanced ranking](#3.3.-advanced-ranking)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.3.1. Reviews' preprocessing](#3.3.1.-reviews'-preprocessing)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.3.2. Calculations of polarity and subjectivity of the reviews with NLP analysis](#3.3.2.-calculations-of-polarity-and-subjectivity-of-the-reviews-with-nlp-analysis)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.3.3. Analysis with the new ranking system](#3.3.3.-analysis-with-the-new-ranking-system)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.3.3.1. New ranking system](#3.3.3.1.-new-ranking-system)  \n",
      "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.3.3.2. Best breweries in the world around the years](#3.3.3.2.-best-breweries-in-the-world-around-the-years)  \n",
      "[4. Annexes](#4.-annexes)  \n",
      "&emsp;&emsp;&emsp;&emsp;[4.1. Generation of the table of content](#4.1.-generation-of-the-table-of-content)  \n"
     ]
    }
   ],
   "source": [
    "print(toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
